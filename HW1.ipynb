{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7252acde",
   "metadata": {},
   "source": [
    "Question 1: [Index] S&P 500 Stocks Added to the Index\n",
    "Which year had the highest number of additions?\n",
    "\n",
    "Using the list of S&P 500 companies from Wikipedia's S&P 500 companies page, download the data including the year each company was added to the index.\n",
    "\n",
    "Hint: you can use pandas.read_html to scrape the data into a DataFrame.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Create a DataFrame with company tickers, names, and the year they were added.\n",
    "Extract the year from the addition date and calculate the number of stocks added each year.\n",
    "Which year had the highest number of additions (1957 doesn't count, as it was the year when the S&P 500 index was founded)? Write down this year as your answer (the most recent one, if you have several records).\n",
    "Context:\n",
    "\n",
    "\"Following the announcement, all four new entrants saw their stock prices rise in extended trading on Friday\" - recent examples of S&P 500 additions include DASH, WSM, EXE, TKO in 2025 (Nasdaq article).\n",
    "\n",
    "Additional: How many current S&P 500 stocks have been in the index for more than 20 years? When stocks are added to the S&P 500, they usually experience a price bump as investors and index funds buy shares following the announcement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8473164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a59afc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YearAdded\n",
      "1957    53\n",
      "2017    23\n",
      "2016    23\n",
      "2019    22\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "df = pd.read_html(url)[0]\n",
    "\n",
    "df1 = df[[\"Symbol\",\"Security\"]].copy() \n",
    "df1[\"YearAdded\"] = pd.to_datetime(df[\"Date added\"]).dt.year\n",
    "print(df1[\"YearAdded\"].value_counts().sort_values(ascending=False).head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98dc457f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219  in last 20 years\n"
     ]
    }
   ],
   "source": [
    "year_diff = (pd.to_datetime('today') - pd.to_datetime(df[\"Date added\"])) / pd.Timedelta(days=365.25)\n",
    "print(len(year_diff[year_diff >= 20]), \" in last 20 years\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ef7dbb",
   "metadata": {},
   "source": [
    "Question 2. [Macro] Indexes YTD (as of 1 May 2025)\n",
    "How many indexes (out of 10) have better year-to-date returns than the US (S&P 500) as of May 1, 2025?\n",
    "\n",
    "Using Yahoo Finance World Indices data, compare the year-to-date (YTD) performance (1 January-1 May 2025) of major stock market indexes for the following countries:\n",
    "\n",
    "United States - S&P 500 (^GSPC)\n",
    "China - Shanghai Composite (000001.SS)\n",
    "Hong Kong - HANG SENG INDEX (^HSI)\n",
    "Australia - S&P/ASX 200 (^AXJO)\n",
    "India - Nifty 50 (^NSEI)\n",
    "Canada - S&P/TSX Composite (^GSPTSE)\n",
    "Germany - DAX (^GDAXI)\n",
    "United Kingdom - FTSE 100 (^FTSE)\n",
    "Japan - Nikkei 225 (^N225)\n",
    "Mexico - IPC Mexico (^MXX)\n",
    "Brazil - Ibovespa (^BVSP)\n",
    "Hint: use start_date='2025-01-01' and end_date='2025-05-01' when downloading daily data in yfinance\n",
    "\n",
    "Context:\n",
    "\n",
    "Global Valuations: Who's Cheap, Who's Not? article suggests \"Other regions may be growing faster than the US and you need to diversify.\"\n",
    "\n",
    "Reference: Yahoo Finance World Indices - https://finance.yahoo.com/world-indices/\n",
    "\n",
    "Additional: How many of these indexes have better returns than the S&P 500 over 3, 5, and 10 year periods? Do you see the same trend? Note: For simplicity, ignore currency conversion effects.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12d3b03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edde39b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  8 of 8 completed\n",
      "[*********************100%***********************]  3 of 3 completed\n"
     ]
    }
   ],
   "source": [
    "## These stocks close on two different dates for 2024, joining them into one\n",
    "df_yearclose1 = yf.download(tickers = [\"^GSPC\", \"^HSI\", \"000001.SS\", \"^AXJO\", \"^NSEI\", \"^GSPTSE\", \"^FTSE\", \"^MXX\"], start = '2024-12-31', end = '2025-01-01', interval = \"1d\")\n",
    "df_yearclose2 = yf.download(tickers = ['^BVSP', '^N225', '^GDAXI'], start = '2024-12-30', end = '2025-01-02', interval = \"1d\")\n",
    "df_yearclose = pd.concat([df_yearclose1, df_yearclose2], ignore_index=True)\n",
    "\n",
    "## Rearranging into a better format\n",
    "yearclose_df = df_yearclose.loc[:, ('Close', slice(None))]\n",
    "yearclose_df.columns = yearclose_df.columns.droplevel()\n",
    "yearclose_df = yearclose_df.stack().reset_index()\n",
    "yearclose_df.columns = ['Date', 'Ticker', 'Close2024']\n",
    "yearclose_df.drop('Date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b11f125d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  11 of 11 completed\n",
      "\n",
      "6 Failed downloads:\n",
      "['^GDAXI', '^MXX', '^HSI', '000001.SS', '^BVSP', '^NSEI']: YFPricesMissingError('possibly delisted; no price data found  (1d 2025-05-01 -> 2025-05-02)')\n",
      "[*********************100%***********************]  6 of 6 completed\n"
     ]
    }
   ],
   "source": [
    "## These stocks close on two different dates for closest to 5/1/2025, joining them into one\n",
    "df_may1 = yf.download(tickers = [\"^GSPC\", \"^HSI\", \"000001.SS\", \"^AXJO\", \"^NSEI\", \"^GSPTSE\", \"^FTSE\", \"^MXX\", '^BVSP', '^N225', '^GDAXI'], start = '2025-05-01', end = '2025-05-02', interval = \"1d\")\n",
    "df_may2 = yf.download(tickers = ['^BVSP', '000001.SS', '^MXX', '^NSEI', '^HSI', '^GDAXI'], start = '2025-04-30', end = '2025-05-01', interval = \"1d\")\n",
    "df_may = pd.concat([df_may1, df_may2], ignore_index=True)\n",
    "\n",
    "## Rearranging into a better format\n",
    "may_df = df_may.loc[:, ('Close', slice(None))]\n",
    "may_df.columns = may_df.columns.droplevel()\n",
    "may_df = may_df.stack().reset_index()\n",
    "may_df.columns = ['Date', 'Ticker', 'May2025']\n",
    "may_df.drop('Date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32e65444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Ticker      Close2024        May2025  percent_change\n",
      "10      ^N225   39894.539062   36452.300781       -0.086283\n",
      "3       ^GSPC    5881.629883    5604.140137       -0.047179\n",
      "0   000001.SS    3351.762939    3279.031006       -0.021700\n",
      "1       ^AXJO    8159.100098    8145.600098       -0.001655\n",
      "4     ^GSPTSE   24727.900391   24795.599609        0.002738\n",
      "7       ^NSEI   23644.800781   24334.199219        0.029156\n",
      "2       ^FTSE    8173.000000    8496.799805        0.039618\n",
      "5        ^HSI   20059.949219   22119.410156        0.102665\n",
      "8       ^BVSP  120283.000000  135067.000000        0.122910\n",
      "9      ^GDAXI   19909.140625   22496.980469        0.129982\n",
      "6        ^MXX   49513.269531   56259.281250        0.136247\n",
      "\n",
      "Count > GSPC: 9\n"
     ]
    }
   ],
   "source": [
    "## Join the two to get initial and final prices and calculate change\n",
    "merged_df = pd.merge(yearclose_df, may_df, on='Ticker')\n",
    "merged_df['percent_change'] = (merged_df['May2025'] - merged_df['Close2024']) / merged_df['Close2024']\n",
    "SP = merged_df[merged_df['Ticker'] == '^GSPC']['percent_change'].values[0]\n",
    "\n",
    "print(merged_df.sort_values(by='percent_change'))\n",
    "print(\"\")\n",
    "print(\"Count > GSPC:\", len(merged_df[merged_df['percent_change'] > SP]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6456fb56",
   "metadata": {},
   "source": [
    "Question 3. [Index] S&P 500 Market Corrections Analysis\n",
    "Calculate the median duration (in days) of significant market corrections in the S&P 500 index.\n",
    "\n",
    "For this task, define a correction as an event when a stock index goes down by more than 5% from the closest all-time high maximum.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Download S&P 500 historical data (1950-present) using yfinance\n",
    "Identify all-time high points (where price exceeds all previous prices)\n",
    "For each pair of consecutive all-time highs, find the minimum price in between\n",
    "Calculate drawdown percentages: (high - low) / high Ã— 100\n",
    "Filter for corrections with at least 5% drawdown\n",
    "Calculate the duration in days for each correction period\n",
    "Determine the 25th, 50th (median), and 75th percentiles for correction durations\n",
    "Context:\n",
    "\n",
    "Investors often wonder about the typical length of market corrections when deciding \"when to buy the dip\" (Reddit discussion).\n",
    "A Wealth of Common Sense - How Often Should You Expect a Stock Market Correction?\n",
    "Hint (use this data to compare with your results): Here is the list of top 10 largest corrections by drawdown:\n",
    "\n",
    "2007-10-09 to 2009-03-09: 56.8% drawdown over 517 days\n",
    "2000-03-24 to 2002-10-09: 49.1% drawdown over 929 days\n",
    "1973-01-11 to 1974-10-03: 48.2% drawdown over 630 days\n",
    "1968-11-29 to 1970-05-26: 36.1% drawdown over 543 days\n",
    "2020-02-19 to 2020-03-23: 33.9% drawdown over 33 days\n",
    "1987-08-25 to 1987-12-04: 33.5% drawdown over 101 days\n",
    "1961-12-12 to 1962-06-26: 28.0% drawdown over 196 days\n",
    "1980-11-28 to 1982-08-12: 27.1% drawdown over 622 days\n",
    "2022-01-03 to 2022-10-12: 25.4% drawdown over 282 days\n",
    "1966-02-09 to 1966-10-07: 22.2% drawdown over 240 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a20669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbec68e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "## This is a more appropriate way of doing it, however it looks like the homework uses close price, not actual high and lows\n",
    "\n",
    "df_sp = yf.download(tickers = [\"^GSPC\"], start = '1950-01-01', end = '2025-06-01', interval = \"1d\")\n",
    "df_sp = df_sp[['High', 'Low']]\n",
    "\n",
    "df_sp.columns = [f'{level}_{col}' for level, col in df_sp.columns]\n",
    "df_sp = df_sp.sort_values(by='Date').reset_index()\n",
    "df_sp.columns = ['Date', 'High', 'Low']\n",
    "\n",
    "df_sp['cummax'] = df_sp['High'].cummax()\n",
    "df_sp['is_cummax'] = df_sp['High'] == df_sp['cummax']\n",
    "\n",
    "df_sp['group'] = df_sp['is_cummax'].cumsum()\n",
    "df_sp['group_min'] = df_sp.groupby('group')['Low'].transform('min')\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8689f85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "Close",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "group_min_day",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "group_min",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "percent_drop",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "day_diff",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "9e8e958a-56d3-42bf-83d9-62ef237b7380",
       "rows": [
        [
         "14534",
         "2007-10-09 00:00:00",
         "1565.1500244140625",
         "2009-03-09 00:00:00",
         "676.530029296875",
         "0.5677538774277282",
         "517"
        ],
        [
         "12639",
         "2000-03-24 00:00:00",
         "1527.4599609375",
         "2002-10-09 00:00:00",
         "776.760009765625",
         "0.49146947898465526",
         "929"
        ],
        [
         "5765",
         "1973-01-11 00:00:00",
         "120.23999786376953",
         "1974-10-03 00:00:00",
         "62.279998779296875",
         "0.4820359290935836",
         "630"
        ],
        [
         "4731",
         "1968-11-29 00:00:00",
         "108.37000274658203",
         "1970-05-26 00:00:00",
         "69.29000091552734",
         "0.3606164145113235",
         "543"
        ],
        [
         "17645",
         "2020-02-19 00:00:00",
         "3386.14990234375",
         "2020-03-23 00:00:00",
         "2237.39990234375",
         "0.3392496000265327",
         "33"
        ],
        [
         "9459",
         "1987-08-25 00:00:00",
         "336.7699890136719",
         "1987-12-04 00:00:00",
         "223.9199981689453",
         "0.33509515255572603",
         "101"
        ],
        [
         "3000",
         "1961-12-12 00:00:00",
         "72.63999938964844",
         "1962-06-26 00:00:00",
         "52.31999969482422",
         "0.2797356809686306",
         "196"
        ],
        [
         "7756",
         "1980-11-28 00:00:00",
         "140.52000427246094",
         "1982-08-12 00:00:00",
         "102.41999816894531",
         "0.2711358165748537",
         "622"
        ],
        [
         "18118",
         "2022-01-03 00:00:00",
         "4796.56005859375",
         "2022-10-12 00:00:00",
         "3577.030029296875",
         "0.254250966192304",
         "282"
        ],
        [
         "4048",
         "1966-02-09 00:00:00",
         "94.05999755859375",
         "1966-10-07 00:00:00",
         "73.19999694824219",
         "0.22177334841367638",
         "240"
        ],
        [
         "1652",
         "1956-08-03 00:00:00",
         "49.63999938964844",
         "1957-10-22 00:00:00",
         "38.97999954223633",
         "0.21474617200811383",
         "445"
        ],
        [
         "10189",
         "1990-07-16 00:00:00",
         "368.95001220703125",
         "1990-10-11 00:00:00",
         "295.4599914550781",
         "0.1991869313469902",
         "87"
        ],
        [
         "17291",
         "2018-09-20 00:00:00",
         "2930.75",
         "2018-12-24 00:00:00",
         "2351.10009765625",
         "0.19778210435681992",
         "95"
        ],
        [
         "12213",
         "1998-07-17 00:00:00",
         "1186.75",
         "1998-08-31 00:00:00",
         "957.280029296875",
         "0.19335999216610492",
         "45"
        ],
        [
         "18902",
         "2025-02-19 00:00:00",
         "6144.14990234375",
         "2025-04-08 00:00:00",
         "4982.77001953125",
         "0.18902206184284007",
         "48"
        ],
        [
         "749",
         "1953-01-05 00:00:00",
         "26.65999984741211",
         "1953-09-14 00:00:00",
         "22.709999084472656",
         "0.14816206997551354",
         "252"
        ],
        [
         "8480",
         "1983-10-10 00:00:00",
         "172.64999389648438",
         "1984-07-24 00:00:00",
         "147.82000732421875",
         "0.14381689805997283",
         "288"
        ],
        [
         "16451",
         "2015-05-21 00:00:00",
         "2130.820068359375",
         "2016-02-11 00:00:00",
         "1829.0799560546875",
         "0.14160750444640421",
         "266"
        ],
        [
         "110",
         "1950-06-12 00:00:00",
         "19.399999618530273",
         "1950-07-17 00:00:00",
         "16.68000030517578",
         "0.14020615292983996",
         "35"
        ],
        [
         "2406",
         "1959-08-03 00:00:00",
         "60.709999084472656",
         "1960-10-25 00:00:00",
         "52.20000076293945",
         "0.1401745750266325",
         "449"
        ],
        [
         "12464",
         "1999-07-16 00:00:00",
         "1418.780029296875",
         "1999-10-15 00:00:00",
         "1247.4100341796875",
         "0.12078686729338568",
         "91"
        ],
        [
         "12018",
         "1997-10-07 00:00:00",
         "983.1199951171875",
         "1997-10-27 00:00:00",
         "876.989990234375",
         "0.10795223920774986",
         "20"
        ],
        [
         "1435",
         "1955-09-23 00:00:00",
         "45.630001068115234",
         "1955-10-11 00:00:00",
         "40.79999923706055",
         "0.10585145119423932",
         "18"
        ],
        [
         "9996",
         "1989-10-09 00:00:00",
         "359.79998779296875",
         "1990-01-30 00:00:00",
         "322.9800109863281",
         "0.1023345693603166",
         "113"
        ],
        [
         "17127",
         "2018-01-26 00:00:00",
         "2872.8701171875",
         "2018-02-08 00:00:00",
         "2581.0",
         "0.10159530548956275",
         "13"
        ],
        [
         "4457",
         "1967-09-25 00:00:00",
         "97.58999633789062",
         "1968-03-05 00:00:00",
         "87.72000122070312",
         "0.1011373653813259",
         "162"
        ],
        [
         "1557",
         "1956-03-20 00:00:00",
         "48.869998931884766",
         "1956-05-28 00:00:00",
         "44.099998474121094",
         "0.09760590468627021",
         "69"
        ],
        [
         "11857",
         "1997-02-18 00:00:00",
         "816.2899780273438",
         "1997-04-11 00:00:00",
         "737.6500244140625",
         "0.09633825690635515",
         "52"
        ],
        [
         "3860",
         "1965-05-13 00:00:00",
         "90.2699966430664",
         "1965-06-28 00:00:00",
         "81.5999984741211",
         "0.0960451810276128",
         "46"
        ],
        [
         "17782",
         "2020-09-02 00:00:00",
         "3580.840087890625",
         "2020-09-23 00:00:00",
         "3236.919921875",
         "0.09604454752912996",
         "21"
        ],
        [
         "14477",
         "2007-07-19 00:00:00",
         "1553.0799560546875",
         "2007-08-15 00:00:00",
         "1406.699951171875",
         "0.09425142878970884",
         "27"
        ],
        [
         "9213",
         "1986-09-04 00:00:00",
         "253.8300018310547",
         "1986-09-29 00:00:00",
         "229.91000366210938",
         "0.09423629199225272",
         "25"
        ],
        [
         "12581",
         "1999-12-31 00:00:00",
         "1469.25",
         "2000-02-25 00:00:00",
         "1333.3599853515625",
         "0.09248937529245363",
         "56"
        ],
        [
         "11089",
         "1994-02-02 00:00:00",
         "482.0",
         "1994-04-04 00:00:00",
         "438.9200134277344",
         "0.08937756550262578",
         "61"
        ],
        [
         "18753",
         "2024-07-16 00:00:00",
         "5667.2001953125",
         "2024-08-05 00:00:00",
         "5186.330078125",
         "0.08485144350207377",
         "20"
        ],
        [
         "333",
         "1951-05-03 00:00:00",
         "22.809999465942383",
         "1951-06-29 00:00:00",
         "20.959999084472656",
         "0.0811047972285998",
         "57"
        ],
        [
         "9361",
         "1987-04-06 00:00:00",
         "301.95001220703125",
         "1987-05-20 00:00:00",
         "278.2099914550781",
         "0.07862235400631758",
         "44"
        ],
        [
         "8927",
         "1985-07-17 00:00:00",
         "195.64999389648438",
         "1985-09-25 00:00:00",
         "180.66000366210938",
         "0.07661635932534704",
         "70"
        ],
        [
         "11672",
         "1996-05-24 00:00:00",
         "678.510009765625",
         "1996-07-24 00:00:00",
         "626.6500244140625",
         "0.07643215959257003",
         "61"
        ],
        [
         "9169",
         "1986-07-02 00:00:00",
         "252.6999969482422",
         "1986-07-15 00:00:00",
         "233.66000366210938",
         "0.07534623472920962",
         "13"
        ],
        [
         "16282",
         "2014-09-18 00:00:00",
         "2011.3599853515625",
         "2014-10-15 00:00:00",
         "1862.489990234375",
         "0.07401459519995708",
         "27"
        ],
        [
         "1470",
         "1955-11-14 00:00:00",
         "46.40999984741211",
         "1956-01-23 00:00:00",
         "43.11000061035156",
         "0.07110534901767641",
         "70"
        ],
        [
         "8248",
         "1982-11-09 00:00:00",
         "143.02000427246094",
         "1982-11-23 00:00:00",
         "132.92999267578125",
         "0.07054965246300554",
         "14"
        ],
        [
         "8404",
         "1983-06-22 00:00:00",
         "170.99000549316406",
         "1983-08-08 00:00:00",
         "159.17999267578125",
         "0.069068439312115",
         "47"
        ],
        [
         "650",
         "1952-08-08 00:00:00",
         "25.549999237060547",
         "1952-10-22 00:00:00",
         "23.799999237060547",
         "0.06849315273018115",
         "75"
        ],
        [
         "17442",
         "2019-04-30 00:00:00",
         "2945.830078125",
         "2019-06-03 00:00:00",
         "2744.449951171875",
         "0.06836108044673847",
         "34"
        ],
        [
         "1294",
         "1955-03-04 00:00:00",
         "37.52000045776367",
         "1955-03-14 00:00:00",
         "34.959999084472656",
         "0.06823031295462839",
         "10"
        ],
        [
         "3473",
         "1963-10-28 00:00:00",
         "74.4800033569336",
         "1963-11-22 00:00:00",
         "69.61000061035156",
         "0.06538671491787287",
         "25"
        ],
        [
         "4360",
         "1967-05-08 00:00:00",
         "94.58000183105469",
         "1967-06-05 00:00:00",
         "88.43000030517578",
         "0.0650243329119877",
         "28"
        ],
        [
         "224",
         "1950-11-24 00:00:00",
         "20.31999969482422",
         "1950-12-04 00:00:00",
         "19.0",
         "0.06496061587837723",
         "10"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 72
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>group_min_day</th>\n",
       "      <th>group_min</th>\n",
       "      <th>percent_drop</th>\n",
       "      <th>day_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14534</th>\n",
       "      <td>2007-10-09</td>\n",
       "      <td>1565.150024</td>\n",
       "      <td>2009-03-09</td>\n",
       "      <td>676.530029</td>\n",
       "      <td>0.567754</td>\n",
       "      <td>517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12639</th>\n",
       "      <td>2000-03-24</td>\n",
       "      <td>1527.459961</td>\n",
       "      <td>2002-10-09</td>\n",
       "      <td>776.760010</td>\n",
       "      <td>0.491469</td>\n",
       "      <td>929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5765</th>\n",
       "      <td>1973-01-11</td>\n",
       "      <td>120.239998</td>\n",
       "      <td>1974-10-03</td>\n",
       "      <td>62.279999</td>\n",
       "      <td>0.482036</td>\n",
       "      <td>630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4731</th>\n",
       "      <td>1968-11-29</td>\n",
       "      <td>108.370003</td>\n",
       "      <td>1970-05-26</td>\n",
       "      <td>69.290001</td>\n",
       "      <td>0.360616</td>\n",
       "      <td>543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17645</th>\n",
       "      <td>2020-02-19</td>\n",
       "      <td>3386.149902</td>\n",
       "      <td>2020-03-23</td>\n",
       "      <td>2237.399902</td>\n",
       "      <td>0.339250</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7709</th>\n",
       "      <td>1980-09-22</td>\n",
       "      <td>130.399994</td>\n",
       "      <td>1980-09-29</td>\n",
       "      <td>123.540001</td>\n",
       "      <td>0.052607</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18034</th>\n",
       "      <td>2021-09-02</td>\n",
       "      <td>4536.950195</td>\n",
       "      <td>2021-10-04</td>\n",
       "      <td>4300.459961</td>\n",
       "      <td>0.052125</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5663</th>\n",
       "      <td>1972-08-14</td>\n",
       "      <td>112.550003</td>\n",
       "      <td>1972-10-16</td>\n",
       "      <td>106.769997</td>\n",
       "      <td>0.051355</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12397</th>\n",
       "      <td>1999-04-12</td>\n",
       "      <td>1358.630005</td>\n",
       "      <td>1999-04-19</td>\n",
       "      <td>1289.479980</td>\n",
       "      <td>0.050897</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10473</th>\n",
       "      <td>1991-08-28</td>\n",
       "      <td>396.640015</td>\n",
       "      <td>1991-10-09</td>\n",
       "      <td>376.799988</td>\n",
       "      <td>0.050020</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date        Close group_min_day    group_min  percent_drop  \\\n",
       "14534 2007-10-09  1565.150024    2009-03-09   676.530029      0.567754   \n",
       "12639 2000-03-24  1527.459961    2002-10-09   776.760010      0.491469   \n",
       "5765  1973-01-11   120.239998    1974-10-03    62.279999      0.482036   \n",
       "4731  1968-11-29   108.370003    1970-05-26    69.290001      0.360616   \n",
       "17645 2020-02-19  3386.149902    2020-03-23  2237.399902      0.339250   \n",
       "...          ...          ...           ...          ...           ...   \n",
       "7709  1980-09-22   130.399994    1980-09-29   123.540001      0.052607   \n",
       "18034 2021-09-02  4536.950195    2021-10-04  4300.459961      0.052125   \n",
       "5663  1972-08-14   112.550003    1972-10-16   106.769997      0.051355   \n",
       "12397 1999-04-12  1358.630005    1999-04-19  1289.479980      0.050897   \n",
       "10473 1991-08-28   396.640015    1991-10-09   376.799988      0.050020   \n",
       "\n",
       "       day_diff  \n",
       "14534       517  \n",
       "12639       929  \n",
       "5765        630  \n",
       "4731        543  \n",
       "17645        33  \n",
       "...         ...  \n",
       "7709          7  \n",
       "18034        32  \n",
       "5663         63  \n",
       "12397         7  \n",
       "10473        42  \n",
       "\n",
       "[72 rows x 6 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sp = yf.download(tickers = [\"^GSPC\"], start = '1950-01-01', end = '2025-06-01', interval = \"1d\")\n",
    "df_sp = df_sp['Close']\n",
    "\n",
    "df_sp = df_sp.sort_values(by='Date').reset_index()\n",
    "df_sp.columns = ['Date', 'Close']\n",
    "\n",
    "df_sp['cummax'] = df_sp['Close'].cummax()\n",
    "df_sp['is_cummax'] = df_sp['Close'] == df_sp['cummax']\n",
    "\n",
    "df_sp['group'] = df_sp['is_cummax'].cumsum()\n",
    "df_sp['group_min'] = df_sp.groupby('group')['Close'].transform('min')\n",
    "df_sp['group_min_day'] =  np.where(df_sp['group_min'] == df_sp['Close'], df_sp['Date'], None)\n",
    "df_sp['group_min_day'] = pd.to_datetime(df_sp['group_min_day'])\n",
    "df_sp['group_min_day'] = df_sp.groupby('group')['group_min_day'].transform('min')\n",
    "\n",
    "df_sp_filtered = df_sp[df_sp['is_cummax']]\n",
    "\n",
    "df_sp_filtered = df_sp_filtered[['Date', 'Close','group_min_day','group_min']]          \n",
    "df_sp_filtered['percent_drop'] = (df_sp_filtered['Close'] - df_sp_filtered['group_min']) / df_sp_filtered['Close']\n",
    "df_sp_filtered['day_diff'] = (df_sp_filtered['group_min_day'] - df_sp_filtered['Date']).dt.days\n",
    "df_sp_filtered = df_sp_filtered[df_sp_filtered['percent_drop'] > .05]\n",
    "\n",
    "print(df_sp_filtered['day_diff'].median())\n",
    "df_sp_filtered.sort_values(by='percent_drop', ascending=False)\n",
    "\n",
    "# Maybe a rounding difference somewhere? I get an even number of observation (72) with the median beiing the average of 39 and 42 = 40.5.\n",
    "# The only answer close is 39\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5282e5c3",
   "metadata": {},
   "source": [
    "Question 4. [Stocks] Earnings Surprise Analysis for Amazon (AMZN)\n",
    "Calculate the median 2-day percentage change in stock prices following positive earnings surprises days.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Load earnings data from CSV (ha1_Amazon.csv) containing earnings dates, EPS estimates, and actual EPS. Make sure you are using the correct delimiter to read the data, such as in this command python pandas.read_csv(\"ha1_Amazon.csv\", delimiter=';') \n",
    "Download complete historical price data using yfinance\n",
    "Calculate 2-day percentage changes for all historical dates: for each sequence of 3 consecutive trading days (Day 1, Day 2, Day 3), compute the return as Close_Day3 / Close_Day1 - 1. (Assume Day 2 may correspond to the earnings announcement.)\n",
    "Identify positive earnings surprises (where \"actual EPS > estimated EPS\" OR \"Surprise (%)>0\")\n",
    "Calculate 2-day percentage changes following positive earnings surprises. Show your answer in % (closest number to the 2nd digit): return * 100.0\n",
    "(Optional) Compare the median 2-day percentage change for positive surprises vs. all historical dates. Do you see the difference?\n",
    "Context: Earnings announcements, especially when they exceed analyst expectations, can significantly impact stock prices in the short term.\n",
    "\n",
    "Reference: Yahoo Finance earnings calendar - https://finance.yahoo.com/calendar/earnings?symbol=AMZN\n",
    "\n",
    "Additional: Is there a correlation between the magnitude of the earnings surprise and the stock price reaction? Does the market react differently to earnings surprises during bull vs. bear markets?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bac8d727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae6939f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_7836\\1925345996.py:5: FutureWarning: Parsed string \"April 29, 2026 at 6 AM EST\" included an un-recognized timezone \"EST\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  df_earnings['Date'] = pd.to_datetime(df_earnings['Earnings Date'].str.replace('EDT', 'EST', regex=False)).dt.normalize()\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.010405227919253135"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_earnings = pd.read_csv(\"ha1_Amazon.csv\", delimiter=';') \n",
    "df_earnings['EPS Estimate (Cleaned)'] = pd.to_numeric(df_earnings['EPS Estimate'].astype(str).str.replace(r'^(?!-)(\\D+)|[^\\d\\.-]+', '', regex=True), errors='coerce')\n",
    "df_earnings['Reported EPS (Cleaned)'] = pd.to_numeric(df_earnings['Reported EPS'].astype(str).str.replace(r'^(?!-)(\\D+)|[^\\d\\.-]+', '', regex=True), errors='coerce')\n",
    "df_earnings['Surprise (%) (Cleaned)'] = pd.to_numeric(df_earnings['Surprise (%)'], errors='coerce')\n",
    "df_earnings['Date'] = pd.to_datetime(df_earnings['Earnings Date'].str.replace('EDT', 'EST', regex=False)).dt.normalize()\n",
    "\n",
    "\n",
    "df_prices = yf.download(tickers = [\"AMZN\"], interval = \"1d\")['Close']\n",
    "df_prices = df_prices.sort_values(by='Date').reset_index()\n",
    "df_prices.columns = ['Date', 'Close']\n",
    "df_prices['two_day_percent'] = (df_prices['Close'].shift(-1) / df_prices['Close'].shift(1)) - 1\n",
    "\n",
    "\n",
    "df_earnings['BeatEarning'] = (df_earnings['Surprise (%) (Cleaned)'] > 0) | (df_earnings['Reported EPS (Cleaned)'] > df_earnings['EPS Estimate (Cleaned)'])\n",
    "merged_df = pd.merge(df_earnings, df_prices, on='Date')\n",
    "\n",
    "merged_df[merged_df['BeatEarning']]['two_day_percent'].median()\n",
    "#get .010405 -- which is apparently wrong?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
